{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11635539,"sourceType":"datasetVersion","datasetId":7300503},{"sourceId":11640878,"sourceType":"datasetVersion","datasetId":7304451}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport csv\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\n# Load pre-trained BLIP model and processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\ndef generate_description(image_path):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(image, return_tensors=\"pt\")\n        out = model.generate(**inputs)\n        description = processor.decode(out[0], skip_special_tokens=True)\n        return description\n    except Exception as e:\n        print(f\"Error processing {image_path}: {e}\")\n        return \"\"\n\ndef create_csv_from_images(image_folder, label_folder, output_csv_path):\n    rows = [[\"image_path\", \"label_path\", \"text\"]]\n    \n    for filename in sorted(os.listdir(image_folder)):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n            image_path = os.path.join(image_folder, filename)\n            image_id = os.path.splitext(filename)[0]\n            \n            # Assuming label file has same name as image file (can adjust extension if needed)\n            label_path = os.path.join(label_folder, f\"{image_id}.jpg\")  # Change extension if label files differ\n\n            caption = generate_description(image_path)\n            rows.append([image_path, label_path, caption])  # Now includes full paths and caption\n\n    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        writer.writerows(rows)\n    print(f\"CSV saved to {output_csv_path}\")\n\n# Example usage\nimage_folder = \"/kaggle/input/final-dataset/photo/photo\"\nlabel_folder = \"/kaggle/input/final-dataset/sketch/sketch\"  # Adjust based on your label location\noutput_csv = \"/kaggle/working/image_caption_with_paths.csv\"\ncreate_csv_from_images(image_folder, label_folder, output_csv)","metadata":{"_uuid":"68b95ccc-61d4-46ea-b197-009f724081cf","_cell_guid":"9dad6765-0b94-4401-b358-06c062e53b83","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your CSV file\ndf = pd.read_csv('/kaggle/input/newversion/image_caption.csv')  # Replace with your actual CSV file path\n\n# Split the data into train (80%) and test (20%)\ntrain_df, test_df = train_test_split(df, test_size=0.05, random_state=42, shuffle=True)\n\n\nprint(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n\ntrain_df['image_path'] = train_df['image_path'].str.lower()\ntrain_df['label_path'] = train_df['label_path'].str.lower()","metadata":{"_uuid":"0634fbac-dba0-4e04-b654-49ab1d4bcb44","_cell_guid":"c8354f12-5dd7-4969-875d-58b2bd666872","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-08T08:29:59.370654Z","iopub.execute_input":"2025-05-08T08:29:59.371324Z","iopub.status.idle":"2025-05-08T08:29:59.403051Z","shell.execute_reply.started":"2025-05-08T08:29:59.371296Z","shell.execute_reply":"2025-05-08T08:29:59.402311Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport random\n\nclass AddGaussianNoise:\n    def __init__(self, mean=0.0, std=0.05, p=0.5):\n        self.mean = mean\n        self.std = std\n        self.p = p\n\n    def __call__(self, tensor):\n        if random.random() < self.p:\n            noise = torch.randn_like(tensor) * self.std + self.mean\n            return tensor + noise\n        return tensor\n\nimport cv2\nimport numpy as np\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass EnhanceCLAHEGamma:\n    def __init__(self, gamma=1.0, clip_limit=2.0, tile_grid_size=(8, 8)):\n        self.gamma = gamma\n        self.clip_limit = clip_limit\n        self.tile_grid_size = tile_grid_size\n\n    def __call__(self, img):\n        img_cv = np.array(img)  # PIL to NumPy\n        if len(img_cv.shape) == 2:  # Grayscale to BGR\n            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_GRAY2BGR)\n        elif img_cv.shape[2] == 4:\n            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGBA2BGR)\n        else:\n            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR)\n\n        # CLAHE\n        lab = cv2.cvtColor(img_cv, cv2.COLOR_BGR2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        clahe_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n\n        # Gamma correction\n        invGamma = 1.0 / self.gamma\n        table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(256)]).astype(\"uint8\")\n        gamma_corrected = cv2.LUT(clahe_img, table)\n\n        # Convert back to PIL\n        final_img = cv2.cvtColor(gamma_corrected, cv2.COLOR_BGR2RGB)\n        return Image.fromarray(final_img)","metadata":{"_uuid":"e8d75452-9006-4f36-bc35-ce49093b5942","_cell_guid":"089ecdea-876b-41a0-899c-0eab5b5f2451","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-08T08:30:07.734983Z","iopub.execute_input":"2025-05-08T08:30:07.735508Z","iopub.status.idle":"2025-05-08T08:30:07.744582Z","shell.execute_reply.started":"2025-05-08T08:30:07.735486Z","shell.execute_reply":"2025-05-08T08:30:07.743997Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"05111828-fd38-4a9a-b1b2-8d7e80b40fcc","_cell_guid":"d0016a02-a313-4d17-8e18-1d631028a806","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-04T04:17:38.864538Z","iopub.execute_input":"2025-05-04T04:17:38.865239Z","iopub.status.idle":"2025-05-04T04:17:38.875913Z","shell.execute_reply.started":"2025-05-04T04:17:38.865186Z","shell.execute_reply":"2025-05-04T04:17:38.875230Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, dataframe, transform=None, label_transform=None):\n        self.data = dataframe.reset_index(drop=True)\n        self.transform = transform\n        self.label_transform = label_transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.data.loc[idx, 'image_path']).convert('RGB')\n        label = Image.open(self.data.loc[idx, 'label_path'])  # grayscale or RGB depending on your use case\n     \n        if self.transform:\n            image = self.transform(image)\n        if self.label_transform:\n            label = self.label_transform(label)\n        else:\n            label = transforms.ToTensor()(label)  # default label tensor conversion\n\n        caption = self.data.loc[idx, 'text']\n\n        return image, label, caption\n\n\n# Define transforms (same as earlier)\nimage_transform = transforms.Compose([\n    transforms.Resize((256, 256)),    # Ensures square aspect ratio\n    EnhanceCLAHEGamma(gamma=0.8),   # You can tweak gamma value\n    transforms.ToTensor(),\n    AddGaussianNoise(std=0.03, p=0.5),  # After ToTensor\n    transforms.Normalize(mean=[0.5, 0.5, 0.5],  # GAN-friendly [-1,1] range\n    std=[0.5, 0.5, 0.5])\n   \n])\n\nlabel_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3-channel\n    transforms.ToTensor()\n])\n\n# Create datasets from DataFrames\ntrain_dataset = CustomImageDataset(train_df, transform=image_transform, label_transform=label_transform)\ntest_dataset = CustomImageDataset(test_df, transform=image_transform, label_transform=label_transform)\n\n    \nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,)","metadata":{"_uuid":"f13ddb31-6009-46de-9576-96f177e9aff0","_cell_guid":"682d8014-7fb7-4352-8141-49f4eec4bafb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-08T08:30:23.900230Z","iopub.execute_input":"2025-05-08T08:30:23.900907Z","iopub.status.idle":"2025-05-08T08:30:23.909512Z","shell.execute_reply.started":"2025-05-08T08:30:23.900881Z","shell.execute_reply":"2025-05-08T08:30:23.908799Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom transformers import CLIPModel, CLIPTokenizer\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        \n        # Image encoder with partial fine-tuning\n        self.image_encoder = models.mobilenet_v3_small(pretrained=True)\n        self.image_encoder.classifier = nn.Identity()\n        \n        # Freeze early layers, allow later layers to fine-tune\n        for i, param in enumerate(self.image_encoder.parameters()):\n            if i < 100:  # Freeze first 100 layers\n                param.requires_grad = False\n        \n        # CLIP model with gradient checkpointing\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Freeze CLIP entirely as it's already well-trained\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n            \n        # Projection layer with regularization\n        self.image_projection = nn.Sequential(\n            nn.Linear(576, 512),\n            nn.Dropout(0.1),\n            nn.LayerNorm(512)\n        )\n\n    def forward(self, image, text):\n        # Image features with dropout during training\n        image_features = self.image_encoder(image)\n        image_features = self.image_projection(image_features)\n        \n        # Text features\n        with torch.no_grad():  # No gradients for CLIP\n            text_inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n            text_inputs = {k: v.to(image.device) for k, v in text_inputs.items()}\n            text_features = self.clip_model.get_text_features(**text_inputs)\n        \n        return image_features, text_features\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AttributeAttention(nn.Module):\n    def __init__(self, feature_dim=512):\n        super(AttributeAttention, self).__init__()\n        \n        # Project image and text features separately\n        self.image_proj = nn.Linear(feature_dim, feature_dim)\n        self.text_proj = nn.Linear(feature_dim, feature_dim)\n        \n        # Layer Normalization for stability\n        self.layer_norm = nn.LayerNorm(feature_dim)\n        \n        # Attention scoring network\n        self.attention_layer = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(feature_dim // 2, 1)\n        )\n\n    def forward(self, image_features, text_features):\n        # Project features first\n        image_proj = self.image_proj(image_features)\n        text_proj = self.text_proj(text_features)\n        \n        # Combine (add) and normalize\n        combined = self.layer_norm(image_proj + text_proj)\n        \n        # Attention scores\n        attention_scores = self.attention_layer(combined)  # (batch_size, 1)\n        \n        # Softmax over batch or feature dimension depending on input shape\n        attention_weights = F.softmax(attention_scores.squeeze(-1), dim=-1)  # (batch_size,)\n        \n        # Reshape attention weights for broadcasting\n        attention_weights = attention_weights.unsqueeze(-1)\n        \n        # Weighted sum of image features\n        attended = image_features * attention_weights\n        \n        # (Optional) Sum across batch if you want a single vector\n        # attended = attended.sum(dim=0)\n        \n        return attended.unsqueeze(-1).unsqueeze(-1)  # For CNN compatibility\n\n\nclass MultiModalFusion(nn.Module):\n    def __init__(self):\n        super(MultiModalFusion, self).__init__()\n        self.feature_extractor = FeatureExtractor()\n        self.attention = AttributeAttention()\n        self.dropout = nn.Dropout2d(0.1)\n\n    def forward(self, image, text):\n        img_feat, txt_feat = self.feature_extractor(image, text)\n        attended_feat = self.attention(img_feat, txt_feat)\n        fused_feat = attended_feat.expand(-1, -1, 16, 16)\n        return self.dropout(fused_feat)\n\n\nclass CoarseGenerator(nn.Module):\n    def __init__(self):\n        super(CoarseGenerator, self).__init__()\n        \n        # Encoder with spectral normalization\n        self.enc1 = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)),\n            nn.LeakyReLU(0.2),\n            nn.Dropout2d(0.1)\n        )\n        self.enc2 = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2)\n        )\n\n        # Decoder with skip connections\n        self.dec1 = nn.Sequential(\n            nn.utils.spectral_norm(nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)),\n            nn.InstanceNorm2d(64),\n            nn.ReLU()\n        )\n        self.dec2 = nn.Sequential(\n            nn.utils.spectral_norm(nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1)),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        if x.dim() == 3:\n            x = x.unsqueeze(0)\n\n        e1 = self.enc1(x)\n        e2 = self.enc2(e1)\n\n        d1 = self.dec1(e2)\n        d1_cat = torch.cat([d1, e1], dim=1)\n        \n        return self.dec2(d1_cat)\n\n \n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(channels, channels, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(),\n            nn.Dropout2d(0.1),\n            nn.utils.spectral_norm(nn.Conv2d(channels, channels, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(SEBlock, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        scale = self.fc(self.global_avg_pool(x))\n        return x * scale\n\nclass DilatedResidualBlock(nn.Module):\n    def __init__(self, channels, dilation=2):\n        super(DilatedResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.utils.spectral_norm(\n                nn.Conv2d(channels, channels, kernel_size=3, padding=dilation, dilation=dilation)\n            ),\n            nn.InstanceNorm2d(channels),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.utils.spectral_norm(\n                nn.Conv2d(channels, channels, kernel_size=3, padding=dilation, dilation=dilation)\n            ),\n            nn.InstanceNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\nclass RefinementGenerator(nn.Module):\n    def __init__(self):\n        super(RefinementGenerator, self).__init__()\n\n        # Initial reduction\n        self.initial = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(515, 256, kernel_size=1)),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout2d(0.2)\n        )\n\n        # Encoder with improved residuals and SE attention\n        self.encoder = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(256, 128, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            DilatedResidualBlock(128, dilation=2),\n            SEBlock(128),\n\n            nn.utils.spectral_norm(nn.Conv2d(128, 64, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            DilatedResidualBlock(64, dilation=2),\n            SEBlock(64)\n        )\n\n        # Output layer\n        self.output_layer = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(64, 3, kernel_size=3, padding=1)),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.initial(x)\n        x = self.encoder(x)\n        return self.output_layer(x)\n\n\nimport torch\nimport torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Full Image Discriminator (global discrimination)\n        self.model = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)),\n            nn.LeakyReLU(0.2),\n            nn.Dropout2d(0.1),\n            \n            nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2),\n            \n            nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout2d(0.1),\n            \n            # Final fully connected layer for the global real/fake prediction\n            nn.utils.spectral_norm(nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0)),\n            nn.Sigmoid()  # Output between 0 and 1 for full image real/fake\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass DAASS(nn.Module):\n    def __init__(self):\n        super(DAASS, self).__init__()\n        \n        # Modules with initialization\n        self.fusion = MultiModalFusion()\n        self.coarse_gen = CoarseGenerator()\n        self.refine_gen = RefinementGenerator()\n        \n        # Initialize weights\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, image, text):\n        # Feature fusion\n        fused_features = self.fusion(image, text)\n        \n        # Coarse generation\n        coarse_sketch = self.coarse_gen(image)\n        \n        # Prepare refinement input\n        fused_features_upsampled = F.interpolate(\n            fused_features,\n            size=coarse_sketch.shape[2:],\n            mode='bilinear',\n            align_corners=False\n        )\n        \n        # Refinement\n        combined_input = torch.cat([coarse_sketch, fused_features_upsampled], dim=1)\n        refined_sketch = self.refine_gen(combined_input)\n        \n        return refined_sketch","metadata":{"_uuid":"5c1d87b1-677d-4681-af22-6ff167cf1d04","_cell_guid":"357b6610-d938-4657-846d-3e2ecb8f7df2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T06:03:25.266766Z","iopub.execute_input":"2025-05-03T06:03:25.267399Z","iopub.status.idle":"2025-05-03T06:03:25.295669Z","shell.execute_reply.started":"2025-05-03T06:03:25.267372Z","shell.execute_reply":"2025-05-03T06:03:25.294882Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom transformers import CLIPModel, CLIPTokenizer\n\n# FeatureExtractor (unchanged)\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        self.image_encoder = models.mobilenet_v3_small(pretrained=True)\n        self.image_encoder.classifier = nn.Identity()\n        for i, param in enumerate(self.image_encoder.parameters()):\n            if i < 100:\n                param.requires_grad = False\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n        self.image_projection = nn.Sequential(\n            nn.Linear(576, 512),\n            nn.Dropout(0.1),\n            nn.LayerNorm(512)\n        )\n\n    def forward(self, image, text):\n        image_features = self.image_encoder(image)\n        image_features = self.image_projection(image_features)\n        with torch.no_grad():\n            text_inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n            text_inputs = {k: v.to(image.device) for k, v in text_inputs.items()}\n            text_features = self.clip_model.get_text_features(**text_inputs)\n        return image_features, text_features\n\n# AttributeAttention (unchanged)\nclass AttributeAttention(nn.Module):\n    def __init__(self, feature_dim=512):\n        super(AttributeAttention, self).__init__()\n        self.image_proj = nn.Linear(feature_dim, feature_dim)\n        self.text_proj = nn.Linear(feature_dim, feature_dim)\n        self.layer_norm = nn.LayerNorm(feature_dim)\n        self.attention_layer = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(feature_dim // 2, 1)\n        )\n\n    def forward(self, image_features, text_features):\n        image_proj = self.image_proj(image_features)\n        text_proj = self.text_proj(text_features)\n        combined = self.layer_norm(image_proj + text_proj)\n        attention_scores = self.attention_layer(combined)\n        attention_weights = F.softmax(attention_scores.squeeze(-1), dim=-1)\n        attention_weights = attention_weights.unsqueeze(-1)\n        attended = image_features * attention_weights\n        return attended.unsqueeze(-1).unsqueeze(-1)\n\n# MultiModalFusion (unchanged)\nclass MultiModalFusion(nn.Module):\n    def __init__(self):\n        super(MultiModalFusion, self).__init__()\n        self.feature_extractor = FeatureExtractor()\n        self.attention = AttributeAttention()\n        self.dropout = nn.Dropout2d(0.2)\n\n    def forward(self, image, text):\n        img_feat, txt_feat = self.feature_extractor(image, text)\n        attended_feat = self.attention(img_feat, txt_feat)\n        fused_feat = F.interpolate(attended_feat, size=image.shape[2:], mode='bilinear', align_corners=False)\n        return self.dropout(fused_feat)\n\n# ResidualBlock (unchanged)\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(channels, channels, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(),\n            nn.Dropout2d(0.1),\n            nn.utils.spectral_norm(nn.Conv2d(channels, channels, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n# SEBlock (unchanged)\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(SEBlock, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        scale = self.fc(self.global_avg_pool(x))\n        return x * scale\n\n# DilatedResidualBlock (unchanged)\nclass DilatedResidualBlock(nn.Module):\n    def __init__(self, channels, dilation=2):\n        super(DilatedResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.utils.spectral_norm(\n                nn.Conv2d(channels, channels, kernel_size=3, padding=dilation, dilation=dilation)\n            ),\n            nn.InstanceNorm2d(channels),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.utils.spectral_norm(\n                nn.Conv2d(channels, channels, kernel_size=3, padding=dilation, dilation=dilation)\n            ),\n            nn.InstanceNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\n# RefinementGenerator (unchanged)\nclass RefinementGenerator(nn.Module):\n    def __init__(self):\n        super(RefinementGenerator, self).__init__()\n        self.initial = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(515, 256, kernel_size=1)),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout2d(0.2)\n        )\n        self.encoder = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(256, 128, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            DilatedResidualBlock(128, dilation=2),\n            SEBlock(128),\n            nn.utils.spectral_norm(nn.Conv2d(128, 64, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            DilatedResidualBlock(64, dilation=2),\n            SEBlock(64)\n        )\n        self.output_layer = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(64, 3, kernel_size=3, padding=1)),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.initial(x)\n        x = self.encoder(x)\n        return self.output_layer(x)\n\n# Discriminator (unchanged)\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)),\n            nn.LeakyReLU(0.2),\n            nn.Dropout2d(0.1),\n            nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout2d(0.2),\n            nn.utils.spectral_norm(nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0)),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# RSUBlock (unchanged)\nclass RSUBlock(nn.Module):\n    \"\"\"Residual U-block (RSU) for U^2-Net.\"\"\"\n    def __init__(self, in_channels, mid_channels, out_channels, num_layers=4):\n        super(RSUBlock, self).__init__()\n        self.in_channels = in_channels\n        self.mid_channels = mid_channels\n        self.out_channels = out_channels\n        self.num_layers = num_layers\n\n        # Initial convolution\n        self.conv1 = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        # Encoder path\n        self.encoder = nn.ModuleList()\n        current_channels = out_channels\n        encoder_channels = [mid_channels] + [mid_channels * 2] * (num_layers - 1)\n        for i in range(num_layers):\n            self.encoder.append(\n                nn.Sequential(\n                    nn.utils.spectral_norm(nn.Conv2d(current_channels, encoder_channels[i], kernel_size=3, padding=1)),\n                    nn.InstanceNorm2d(encoder_channels[i]),\n                    nn.LeakyReLU(0.2, inplace=True)\n                )\n            )\n            if i < num_layers - 1:\n                self.encoder.append(nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True))\n            current_channels = encoder_channels[i]\n\n        # Dilated convolution at bottleneck\n        self.bottleneck = nn.Sequential(\n            nn.utils.spectral_norm(\n                nn.Conv2d(current_channels, current_channels, kernel_size=3, padding=2, dilation=2)\n            ),\n            nn.InstanceNorm2d(current_channels),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        # Decoder path (only conv layers, upsampling via F.interpolate in forward)\n        self.decoder = nn.ModuleList()\n        decoder_in_channels = [current_channels] + [mid_channels * 2] * (num_layers - 2)\n        decoder_out_channels = [mid_channels * 2] * (num_layers - 2) + [mid_channels]\n        encoder_skip_channels = encoder_channels[:-1][::-1]\n        for i in range(num_layers - 1):\n            self.decoder.append(\n                nn.Sequential(\n                    nn.utils.spectral_norm(\n                        nn.Conv2d(decoder_in_channels[i] + encoder_skip_channels[i], decoder_out_channels[i], kernel_size=3, padding=1)\n                    ),\n                    nn.InstanceNorm2d(decoder_out_channels[i]),\n                    nn.LeakyReLU(0.2, inplace=True)\n                )\n            )\n\n        # Final convolution\n        self.conv_out = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(mid_channels + out_channels, out_channels, kernel_size=3, padding=1)),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n    def forward(self, x):\n        hx = self.conv1(x)\n        hxs = [hx]\n        for i in range(self.num_layers):\n            hx = self.encoder[i * 2](hx)\n            if i < self.num_layers - 1:\n                hx = self.encoder[i * 2 + 1](hx)\n            hxs.append(hx)\n        hx = self.bottleneck(hxs[-1])\n        for i in range(self.num_layers - 1):\n            # Upsample to match the skip connection size\n            hx = F.interpolate(hx, size=hxs[-(i + 2)].shape[2:], mode='bilinear', align_corners=False)\n            hx = self.decoder[i](torch.cat([hx, hxs[-(i + 2)]], dim=1))\n        # Final upsampling to match hxs[0] size\n        hx = F.interpolate(hx, size=hxs[0].shape[2:], mode='bilinear', align_corners=False)\n        output = self.conv_out(torch.cat([hx, hxs[0]], dim=1))\n        return output + hxs[0]\n\n# Updated U2NetCoarseGenerator\nclass U2NetCoarseGenerator(nn.Module):\n    \"\"\"U^2-Net-based coarse generator.\"\"\"\n    def __init__(self, model_type='u2net'):\n        super(U2NetCoarseGenerator, self).__init__()\n        self.model_type = model_type\n\n        # Channel configurations\n        if model_type == 'u2net':\n            cfg = [64, 128, 256, 512, 512, 512]\n            mid_cfg = [32, 32, 64, 128, 256, 256]\n        else:  # u2netp\n            cfg = [16, 32, 64, 128, 256, 256]\n            mid_cfg = [16, 16, 32, 64, 128, 128]\n\n        # Encoder stages\n        self.stage1 = RSUBlock(3, mid_cfg[0], cfg[0], num_layers=4)\n        self.pool12 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.stage2 = RSUBlock(cfg[0], mid_cfg[1], cfg[1], num_layers=4)\n        self.pool23 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.stage3 = RSUBlock(cfg[1], mid_cfg[2], cfg[2], num_layers=4)\n        self.pool34 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.stage4 = RSUBlock(cfg[2], mid_cfg[3], cfg[3], num_layers=3)\n        self.pool45 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.stage5 = RSUBlock(cfg[3], mid_cfg[4], cfg[4], num_layers=3)\n        self.pool56 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n        self.stage6 = RSUBlock(cfg[4], mid_cfg[5], cfg[5], num_layers=3)\n\n        # Decoder stages\n        self.stage5d = RSUBlock(cfg[4], mid_cfg[4], cfg[4], num_layers=3)\n        self.stage4d = RSUBlock(cfg[3], mid_cfg[3], cfg[3], num_layers=3)\n        self.stage3d = RSUBlock(cfg[2], mid_cfg[2], cfg[2], num_layers=4)\n        self.stage2d = RSUBlock(cfg[1], mid_cfg[1], cfg[1], num_layers=4)\n        self.stage1d = RSUBlock(cfg[0], mid_cfg[0], cfg[0], num_layers=4)\n\n        # Channel reduction convolutions for decoder skip connections\n        self.conv5d = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(cfg[5], cfg[4], kernel_size=1)),\n            nn.InstanceNorm2d(cfg[4]),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        self.conv4d = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(cfg[4], cfg[3], kernel_size=1)),\n            nn.InstanceNorm2d(cfg[3]),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        self.conv3d = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(cfg[3], cfg[2], kernel_size=1)),\n            nn.InstanceNorm2d(cfg[2]),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        self.conv2d = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(cfg[2], cfg[1], kernel_size=1)),\n            nn.InstanceNorm2d(cfg[1]),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        self.conv1d = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(cfg[1], cfg[0], kernel_size=1)),\n            nn.InstanceNorm2d(cfg[0]),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        # Output layer\n        self.output_layer = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(cfg[0], 3, kernel_size=3, padding=1)),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        if x.dim() == 3:\n            x = x.unsqueeze(0)\n\n        # Encoder path\n        hx1 = self.stage1(x)\n        hx = self.pool12(hx1)\n        hx2 = self.stage2(hx)\n        hx = self.pool23(hx2)\n        hx3 = self.stage3(hx)\n        hx = self.pool34(hx3)\n        hx4 = self.stage4(hx)\n        hx = self.pool45(hx4)\n        hx5 = self.stage5(hx)\n        hx = self.pool56(hx5)\n        hx6 = self.stage6(hx)\n\n        # Decoder path\n        hx6_up = F.interpolate(hx6, size=hx5.shape[2:], mode='bilinear', align_corners=False)\n        hx5d = self.stage5d(self.conv5d(hx6_up) + hx5)\n        hx5d_up = F.interpolate(hx5d, size=hx4.shape[2:], mode='bilinear', align_corners=False)\n        hx4d = self.stage4d(self.conv4d(hx5d_up) + hx4)\n        hx4d_up = F.interpolate(hx4d, size=hx3.shape[2:], mode='bilinear', align_corners=False)\n        hx3d = self.stage3d(self.conv3d(hx4d_up) + hx3)\n        hx3d_up = F.interpolate(hx3d, size=hx2.shape[2:], mode='bilinear', align_corners=False)\n        hx2d = self.stage2d(self.conv2d(hx3d_up) + hx2)\n        hx2d_up = F.interpolate(hx2d, size=hx1.shape[2:], mode='bilinear', align_corners=False)\n        hx1d = self.stage1d(self.conv1d(hx2d_up) + hx1)\n\n        # Output\n        output = self.output_layer(hx1d)\n        return output\n\n# DAASS (unchanged)\nclass DAASS(nn.Module):\n    def __init__(self, coarse_model_type='u2net'):\n        super(DAASS, self).__init__()\n        self.fusion = MultiModalFusion()\n        self.coarse_gen = U2NetCoarseGenerator(model_type=coarse_model_type)\n        self.refine_gen = RefinementGenerator()\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, image, text):\n        fused_features = self.fusion(image, text)\n        coarse_sketch = self.coarse_gen(image)\n        fused_features_upsampled = F.interpolate(\n            fused_features,\n            size=coarse_sketch.shape[2:],\n            mode='bilinear',\n            align_corners=False\n        )\n        combined_input = torch.cat([coarse_sketch, fused_features_upsampled], dim=1)\n        refined_sketch = self.refine_gen(combined_input)\n        return refined_sketch","metadata":{"_uuid":"7638025a-7dc7-48d7-a05c-7ba2a90015b1","_cell_guid":"0ff928ba-5419-48a3-9020-bdbc2059d337","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-08T08:30:27.193029Z","iopub.execute_input":"2025-05-08T08:30:27.193302Z","iopub.status.idle":"2025-05-08T08:30:27.235354Z","shell.execute_reply.started":"2025-05-08T08:30:27.193279Z","shell.execute_reply":"2025-05-08T08:30:27.234737Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"New","metadata":{"_uuid":"ee9afca9-0527-45b5-861f-c0bc94cd7e3a","_cell_guid":"36ee3d1d-4498-478e-b585-cce9b760fd33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchvision.models.feature_extraction import create_feature_extractor\n\nclass VGGPerceptualLoss(nn.Module):\n    def __init__(self, layers=['relu3_3']):\n        super(VGGPerceptualLoss, self).__init__()\n        \n        # Load pre-trained VGG19 model\n        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.eval()\n        for param in vgg.parameters():\n            param.requires_grad = False\n\n        # Map layer index to names\n        layer_name_mapping = {\n            '0': \"conv1_1\", '1': \"relu1_1\",\n            '2': \"conv1_2\", '3': \"relu1_2\",\n            '4': \"pool1\",\n            '5': \"conv2_1\", '6': \"relu2_1\",\n            '7': \"conv2_2\", '8': \"relu2_2\",\n            '9': \"pool2\",\n            '10': \"conv3_1\", '11': \"relu3_1\",\n            '12': \"conv3_2\", '13': \"relu3_2\",\n            '14': \"conv3_3\", '15': \"relu3_3\",\n            '16': \"pool3\",\n            '17': \"conv4_1\", '18': \"relu4_1\",\n            '19': \"conv4_2\", '20': \"relu4_2\",\n            '21': \"conv4_3\", '22': \"relu4_3\",\n            '23': \"pool4\",\n            '24': \"conv5_1\", '25': \"relu5_1\",\n            '26': \"conv5_2\", '27': \"relu5_2\",\n            '28': \"conv5_3\", '29': \"relu5_3\",\n            '30': \"pool5\",\n        }\n\n        # Set which layers to extract\n        return_nodes = {k: v for k, v in layer_name_mapping.items() if v in layers}\n\n        # Create efficient feature extractor\n        self.feature_extractor = create_feature_extractor(vgg, return_nodes=return_nodes)\n\n        # Store selected layers for loss computation\n        self.selected_layers = layers\n\n        # ImageNet normalization\n        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n\n    def normalize(self, x):\n        return (x - self.mean) / self.std\n\n    def forward(self, x, y):\n        x = self.normalize(x)\n        y = self.normalize(y)\n\n        x_feats = self.feature_extractor(x)\n        y_feats = self.feature_extractor(y)\n\n        loss = 0.0\n        for layer in self.selected_layers:\n            loss += nn.functional.l1_loss(x_feats[layer], y_feats[layer])\n        return loss","metadata":{"_uuid":"9fb6cf17-49e4-4d0c-a5fa-3bbd6e0c8d0b","_cell_guid":"ee68ceb7-4797-418b-9734-bb4ca7a76857","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-08T08:30:32.853414Z","iopub.execute_input":"2025-05-08T08:30:32.853690Z","iopub.status.idle":"2025-05-08T08:30:32.862002Z","shell.execute_reply.started":"2025-05-08T08:30:32.853667Z","shell.execute_reply":"2025-05-08T08:30:32.861389Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"a08e5956-8518-4957-9d4a-ccb5b3c58d4e","_cell_guid":"3fc95d5a-0253-4d74-8949-c999fe01fd6d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-30T09:00:35.852641Z","iopub.execute_input":"2025-04-30T09:00:35.852951Z","iopub.status.idle":"2025-04-30T09:00:35.875339Z","shell.execute_reply.started":"2025-04-30T09:00:35.852931Z","shell.execute_reply":"2025-04-30T09:00:35.874571Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport numpy as np\ndef calculate_foreground_accuracy(fake_imgs, real_imgs, threshold=0.3):\n    \"\"\"\n    Calculates accuracy only on the foreground (i.e., non-background) pixels.\n    \"\"\"\n    with torch.no_grad():\n        # Normalize from [-1, 1] to [0, 1]\n        fake_imgs = (fake_imgs + 1) / 2\n        real_imgs = (real_imgs + 1) / 2\n\n        # Binarize\n        fake_bin = (fake_imgs >= threshold).float()\n        real_bin = (real_imgs >= threshold).float()\n\n        # Focus only on pixels where ground truth is foreground (i.e., real_bin == 1)\n        foreground_pixels = (real_bin == 1).float()\n\n        if foreground_pixels.sum() == 0:\n            return torch.tensor(1.0)  # If no foreground in mask, consider it perfect (or handle differently)\n\n        correct_foreground = ((fake_bin == real_bin) * foreground_pixels).sum()\n        total_foreground = foreground_pixels.sum()\n\n        acc = correct_foreground / total_foreground\n    return acc\n\n\ndef calculate_discriminator_accuracy_biased(real_preds, fake_preds, bias=0.05):\n    device = real_preds.device\n\n    with torch.no_grad():\n        threshold_real = 0.5 - bias\n        threshold_fake = 0.5 + bias\n\n        pred_real = (real_preds >= threshold_real).float()\n        pred_fake = (fake_preds < threshold_fake).float()\n\n        correct = pred_real.sum() + pred_fake.sum()\n        total = real_preds.numel() + fake_preds.numel()\n\n        acc = correct / total\n        acc = acc.to(device)\n\n    return acc\n# def calculate_metrics(fake_imgs, real_imgs):\n#     \"\"\"Calculates PSNR and SSIM between fake and real images\"\"\"\n#     psnr_total = 0.0\n#     ssim_total = 0.0\n#     batch_size = fake_imgs.shape[0]\n\n#     for i in range(batch_size):\n#         # Convert torch tensors to numpy, rescale to [0, 1]\n#         fake_np = fake_imgs[i].cpu().detach().numpy().transpose(1, 2, 0)\n#         real_np = real_imgs[i].cpu().detach().numpy().transpose(1, 2, 0)\n\n#         fake_np = ((fake_np + 1) / 2).clip(0, 1)  # Tanh to [0, 1]\n#         real_np = ((real_np + 1) / 2).clip(0, 1)\n\n#         psnr = peak_signal_noise_ratio(real_np, fake_np, data_range=1)\n#         ssim = structural_similarity(real_np, fake_np, data_range=1, win_size=5, channel_axis=-1)\n\n\n#         psnr_total += psnr\n#         ssim_total += ssim\n\n#     return psnr_total / batch_size, ssim_total / batch_size\ndef calculate_metrics(fake_imgs, real_imgs):\n    \"\"\"Calculates PSNR and SSIM between fake and real images\"\"\"\n    psnr_total = 0.0\n    ssim_total = 0.0\n    batch_size = fake_imgs.shape[0]\n\n    for i in range(batch_size):\n        # Convert torch tensors to numpy arrays and rescale to [0, 1]\n        fake_np = fake_imgs[i].cpu().detach().numpy().transpose(1, 2, 0)\n        real_np = real_imgs[i].cpu().detach().numpy().transpose(1, 2, 0)\n\n        # Ensure images are in the range [0, 1] for PSNR and SSIM calculation\n        fake_np = np.clip((fake_np + 1) / 2, 0, 1)  # Tanh to [0, 1]\n        real_np = np.clip((real_np + 1) / 2, 0, 1)\n\n        # Calculate PSNR and SSIM\n        psnr = peak_signal_noise_ratio(real_np, fake_np, data_range=1)\n        ssim = structural_similarity(real_np, fake_np, data_range=1, win_size=5, channel_axis=-1)\n\n        psnr_total += psnr\n        ssim_total += ssim\n\n    # Average PSNR and SSIM over the batch\n    avg_psnr = psnr_total / batch_size\n    avg_ssim = ssim_total / batch_size\n    return avg_psnr, avg_ssim\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef train_daass(model, discriminator, dataloader, optimizer_G, optimizer_D, criterion, device,  scheduler_G=None, scheduler_D=None,num_epochs=100):\n    model.to(device)\n    discriminator.to(device)\n    perceptual_loss = VGGPerceptualLoss().to(device)\n        \n    # Check parameters\n    print(f\"Generator parameters: {count_parameters(generator_model)}\")\n    print(f\"Discriminator parameters: {count_parameters(discriminator_model)}\")\n\n\n    for epoch in range(num_epochs):\n        model.train()\n        discriminator.train()\n\n        total_d_loss = 0.0\n        total_g_loss = 0.0\n        total_psnr = 0.0\n        total_ssim = 0.0\n        total_acc = 0.0\n        total_acc2 = 0.0\n        total_batches = 0\n        \n        print(f\"\\n--- Epoch [{epoch+1}/{num_epochs}] ---\")\n\n        for batch_idx, (images, labels, texts) in enumerate(dataloader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass through generator\n            fake_sketches = model(images, texts)\n\n            # Discriminator predictions\n            real_preds = discriminator(labels)\n            fake_preds = discriminator(fake_sketches.detach())\n\n            # Real and fake labels\n            real_labels = torch.ones_like(real_preds).to(device)\n            fake_labels = torch.zeros_like(fake_preds).to(device)\n\n            # Discriminator loss\n            d_loss_real = criterion(real_preds, real_labels)\n            d_loss_fake = criterion(fake_preds, fake_labels)\n            d_loss = (d_loss_real + d_loss_fake) / 2\n\n            optimizer_D.zero_grad()\n            d_loss.backward()\n            optimizer_D.step()\n\n            # Generator loss\n            fake_preds_for_g = discriminator(fake_sketches)\n            fake_norm = (fake_sketches + 1) / 2\n            real_norm = (labels + 1) / 2\n\n            # Make 3 channels if needed\n            if fake_norm.shape[1] == 1:\n                fake_norm = fake_norm.repeat(1, 3, 1, 1)\n                real_norm = real_norm.repeat(1, 3, 1, 1)\n            \n            g_adv_loss = criterion(fake_preds_for_g, real_labels)\n            p_loss = perceptual_loss(fake_norm, real_norm)\n            g_loss = g_adv_loss + 0.1 * p_loss  # 0.1 is the weight for perceptual loss\n                        #g_loss = criterion(fake_preds_for_g, real_labels)\n            #print(f\"Perceptual Loss: {p_loss.item():.4f}\")\n\n\n            optimizer_G.zero_grad()\n            g_loss.backward()\n            optimizer_G.step()\n\n            # Metrics\n            psnr_score, ssim_score = calculate_metrics(fake_sketches, labels)\n\n            # Discriminator accuracy\n            #pred_real = (real_preds > 0.5).float()\n            #pred_fake = (fake_preds < 0.5).float()\n            #acc = (pred_real.sum() + pred_fake.sum()) / (2 * images.size(0) * real_preds.shape[-2] * real_preds.shape[-1])\n            # Bias real predictions by 5% toward being classified as real\n            acc = calculate_discriminator_accuracy_biased(real_preds, fake_preds, bias=0.05)\n            acc2 = calculate_foreground_accuracy(fake_sketches, labels, threshold=0.5)\n\n\n\n\n            # Accumulate\n            total_d_loss += d_loss.item()\n            total_g_loss += g_loss.item()\n            total_psnr += psnr_score\n            total_ssim += ssim_score\n            total_acc += acc.item()\n            total_acc2 += acc2.item()\n            total_batches += 1\n\n            # # Print batch-level info\n            # if (batch_idx + 1) % 10 == 0 or (batch_idx == len(dataloader) - 1):\n            #     print(f\"[Epoch {epoch+1}/{num_epochs}] Batch {batch_idx+1}/{len(dataloader)} | \"\n            #           f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f} | \"\n            #           f\"PSNR: {psnr_score:.2f} | SSIM: {ssim_score:.3f} | Acc: {acc.item():.3f}\")\n\n        # Print epoch summary\n        avg_d_loss = total_d_loss / total_batches\n        avg_g_loss = total_g_loss / total_batches\n        avg_psnr = total_psnr / total_batches\n        avg_ssim = total_ssim / total_batches\n        avg_acc = total_acc / total_batches\n        avg_acc2 = total_acc2 / total_batches\n\n        print(f\"\\nEpoch {epoch+1} Summary  D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f} | \"\n              f\"PSNR: {avg_psnr:.2f} | SSIM: {avg_ssim:.3f} | Acc: {avg_acc:.3f} | Acc2: {avg_acc2:.3f}\")\n         # Step the learning rate scheduler (after epoch)\n        if scheduler_G is not None:\n            scheduler_G.step()\n        if scheduler_D is not None:\n            scheduler_D.step()","metadata":{"_uuid":"a0b9df0b-4498-49b6-8c41-e6dbc846bb0b","_cell_guid":"e29f3f79-60d1-4852-8a28-3ea3b2383ad6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-08T08:30:40.946309Z","iopub.execute_input":"2025-05-08T08:30:40.946587Z","iopub.status.idle":"2025-05-08T08:30:40.963004Z","shell.execute_reply.started":"2025-05-08T08:30:40.946566Z","shell.execute_reply":"2025-05-08T08:30:40.962309Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate models\ngenerator_model = DAASS().to(device)\ndiscriminator_model = Discriminator().to(device)\n\n# Define loss criterion\ncriterion = nn.BCELoss()  # Binary Cross Entropy Loss for adversarial training\n\n\n\n#optimizer_G = optim.Adam(generator_model.parameters(), lr=0.0002, betas=(0.5, 0.999),weight_decay=1e-5)\n#optimizer_D = optim.Adam(discriminator_model.parameters(), lr=0.0002, betas=(0.5, 0.999),weight_decay=1e-5)\noptimizer_G = optim.AdamW(generator_model.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=1e-4)\noptimizer_D = optim.AdamW(discriminator_model.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=1e-4)\n# Define optimizers\nscheduler_G = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=50)\nscheduler_D = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=50)\n\n# Assuming you already have your DataLoader defined as `train_loader`\n# It should return: images (tensor), labels (target sketch images), texts (list of strings)\n\n# Call training loop\ntrain_daass(\n    model=generator_model,\n    discriminator=discriminator_model,\n    dataloader=train_loader,\n    optimizer_G=optimizer_G,\n    optimizer_D=optimizer_D,\n    criterion=criterion,\n    device=device,\n    scheduler_G=scheduler_G,  #  new\n    scheduler_D=scheduler_D,  #  new\n    num_epochs=20\n)","metadata":{"_uuid":"9f048f97-5826-4e0a-b4f4-13700281e3e2","_cell_guid":"8e668420-5966-4b32-a794-a83b5a64781d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-08T08:31:26.801626Z","iopub.execute_input":"2025-05-08T08:31:26.802206Z","execution_failed":"2025-05-08T08:54:46.450Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef calculate_accuracy(fake, real, threshold=0.3):\n    \"\"\"Pixel-wise accuracy for binary images (or grayscale if thresholded).\"\"\"\n    fake_bin = (fake > threshold).float()\n    real_bin = (real >= threshold).float()\n    correct = (fake_bin == real_bin).float()\n    accuracy = correct.mean().item()\n    return accuracy\n\ndef test_daass(model, discriminator, dataloader, criterion, device):\n    model.eval()  # Set the model to evaluation mode\n    discriminator.eval()\n\n    total_psnr = 0.0\n    total_ssim = 0.0\n    total_accuracy = 0.0\n    total_batches = 0\n\n    with torch.no_grad():\n        print(\"\\n--- Testing ---\")\n\n        for batch_idx, (images, labels, texts) in enumerate(dataloader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass through the generator\n            fake_sketches = model(images, texts)\n\n            # Post-processing: scale Tanh output from [-1, 1] to [0, 1]\n            fake_sketches = (fake_sketches + 1) / 2\n            fake_sketches = fake_sketches.clamp(0, 1)\n\n            # Metrics\n            psnr_score, ssim_score = calculate_metrics(fake_sketches, labels)\n            accuracy_score = calculate_accuracy(fake_sketches, labels)\n\n            total_psnr += psnr_score\n            total_ssim += ssim_score\n            total_accuracy += accuracy_score\n            total_batches += 1\n\n            # Visualize all images in the first batch only\n            if batch_idx == 0:\n                num_images = images.size(0)\n                fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n\n                if num_images == 0:\n                    axes = [axes]  # Handle case when batch size is 1\n\n                for idx in range(num_images):\n                    input_img = images[idx].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n                    real_img = labels[idx].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n                    gen_img = fake_sketches[idx].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n\n                    axes[idx][0].imshow(input_img)\n                    axes[idx][0].set_title(\"Input Image\")\n                    axes[idx][0].axis(\"off\")\n\n                    axes[idx][1].imshow(real_img)\n                    axes[idx][1].set_title(\"Real Sketch\")\n                    axes[idx][1].axis(\"off\")\n\n                    axes[idx][2].imshow(gen_img)\n                    axes[idx][2].set_title(\"Generated Sketch\")\n                    axes[idx][2].axis(\"off\")\n\n                plt.tight_layout()\n                plt.show()\n\n        # Print average metrics\n        avg_psnr = total_psnr / total_batches\n        avg_ssim = total_ssim / total_batches\n        avg_accuracy = total_accuracy / total_batches\n        \n        print(f\"\\nTest Summary  PSNR: {avg_psnr:.2f} | SSIM: {avg_ssim:.3f} | Accuracy: {avg_accuracy:.3f}\")","metadata":{"_uuid":"78ab5b37-576f-46ec-959d-d162f6480a2e","_cell_guid":"6510d012-93d7-4349-a862-979ce0081796","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-06T06:22:01.108816Z","iopub.execute_input":"2025-05-06T06:22:01.109127Z","iopub.status.idle":"2025-05-06T06:22:01.119704Z","shell.execute_reply.started":"2025-05-06T06:22:01.109106Z","shell.execute_reply":"2025-05-06T06:22:01.119179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Assuming you have a test DataLoader called `test_loader`:\n\n# Load model weights (if not already loaded)\n# generator_model.load_state_dict(torch.load('generator_epoch_final.pth'))\n# discriminator_model.load_state_dict(torch.load('discriminator_epoch_final.pth'))\n\n# Test the model\ntest_daass(\n    model=generator_model,\n    discriminator=discriminator_model,\n    dataloader=test_loader,\n    criterion=criterion,\n    device=device\n)","metadata":{"_uuid":"d2ab9689-e958-4d60-ba19-626153f92960","_cell_guid":"fabe5892-de4b-42b0-94db-1ca81199c3ee","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T06:22:07.219012Z","iopub.execute_input":"2025-05-06T06:22:07.219276Z","iopub.status.idle":"2025-05-06T06:22:12.500115Z","shell.execute_reply.started":"2025-05-06T06:22:07.219256Z","shell.execute_reply":"2025-05-06T06:22:12.499359Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Assuming you have a test DataLoader called `test_loader`:\n\n# Load model weights (if not already loaded)\n# generator_model.load_state_dict(torch.load('generator_epoch_final.pth'))\n# discriminator_model.load_state_dict(torch.load('discriminator_epoch_final.pth'))\n\n# Test the model\ntest_daass(\n    model=generator_model,\n    discriminator=discriminator_model,\n    dataloader=test_loader,\n    criterion=criterion,\n    device=device\n)","metadata":{"_uuid":"b5a93a1f-fd21-4ceb-bbb9-c153ad05cebe","_cell_guid":"e2ea0a57-af84-4683-9570-8ae2b9de9eb5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T17:29:48.967759Z","iopub.execute_input":"2025-05-04T17:29:48.968455Z","iopub.status.idle":"2025-05-04T17:29:53.709406Z","shell.execute_reply.started":"2025-05-04T17:29:48.968431Z","shell.execute_reply":"2025-05-04T17:29:53.708746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage\nfrom skimage.feature import local_binary_pattern\nfrom skimage.filters import sobel\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef calculate_vif(fake, real, sigma_n_sq=2.0):\n    \"\"\"Calculate Visual Information Fidelity (VIF) score using a simplified approach.\"\"\"\n    # Convert tensors to numpy arrays, shape (N, H, W, C)\n    fake = fake.cpu().permute(0, 2, 3, 1).numpy()  # (N, H, W, C)\n    real = real.cpu().permute(0, 2, 3, 1).numpy()  # (N, H, W, C)\n    \n    # Convert to grayscale if multi-channel (RGB)\n    if fake.shape[-1] == 3:  # RGB\n        fake = 0.2989 * fake[..., 0] + 0.5870 * fake[..., 1] + 0.1140 * fake[..., 2]  # (N, H, W)\n        real = 0.2989 * real[..., 0] + 0.5870 * real[..., 1] + 0.1140 * real[..., 2]  # (N, H, W)\n    \n    vif_scores = []\n    for i in range(fake.shape[0]):\n        # Apply Gaussian blur to model human visual system\n        fake_blur = ndimage.gaussian_filter(fake[i], sigma=2)\n        real_blur = ndimage.gaussian_filter(real[i], sigma=2)\n        \n        # Compute variance and covariance\n        var_fake = np.var(fake_blur)\n        var_real = np.var(real_blur)\n        cov_fake_real = np.cov(fake_blur.flatten(), real_blur.flatten())[0, 1]\n        \n        # Avoid division by zero\n        if var_real < 1e-10:\n            vif = 0.0\n        else:\n            # Simplified VIF: mutual information approximation\n            vif = np.log2(1 + (var_fake + cov_fake_real) / (var_real + sigma_n_sq))\n        vif_scores.append(vif)\n    \n    return np.mean(vif_scores)\n\n\ndef calculate_fsim(fake, real):\n    \"\"\"Calculate Feature Similarity Index (FSIM) score using gradient and texture features.\"\"\"\n    # Convert tensors to numpy arrays, shape (N, H, W, C)\n    fake = fake.cpu().permute(0, 2, 3, 1).numpy()  # (N, H, W, C)\n    real = real.cpu().permute(0, 2, 3, 1).numpy()  # (N, H, W, C)\n    \n    # Convert to grayscale if multi-channel (RGB)\n    if fake.shape[-1] == 3:  # RGB\n        fake = 0.2989 * fake[..., 0] + 0.5870 * fake[..., 1] + 0.1140 * fake[..., 2]  # (N, H, W)\n        real = 0.2989 * real[..., 0] + 0.5870 * real[..., 1] + 0.1140 * real[..., 2]  # (N, H, W)\n    \n    fsim_scores = []\n    for i in range(fake.shape[0]):\n        fake_img = fake[i]\n        real_img = real[i]\n        \n        # Compute gradient magnitude using Sobel\n        fake_grad = sobel(fake_img)\n        real_grad = sobel(real_img)\n        \n        # Compute texture using Local Binary Pattern (LBP)\n        lbp_fake = local_binary_pattern(fake_img, P=8, R=1, method='uniform')\n        lbp_real = local_binary_pattern(real_img, P=8, R=1, method='uniform')\n        \n        # Gradient similarity\n        grad_sim = (2 * fake_grad * real_grad + 1e-6) / (fake_grad**2 + real_grad**2 + 1e-6)\n        grad_sim = np.mean(grad_sim)\n        \n        # Texture similarity\n        texture_sim = (2 * lbp_fake * lbp_real + 1e-6) / (lbp_fake**2 + lbp_real**2 + 1e-6)\n        texture_sim = np.mean(texture_sim)\n        \n        # FSIM score (weighted combination)\n        fsim = 0.8 * grad_sim + 0.2 * texture_sim\n        fsim_scores.append(fsim)\n    \n    return np.mean(fsim_scores)\n\n\ndef calculate_gsm(fake, real):\n    \"\"\"Calculate Gradient Similarity Metric (GSM) based on gradient magnitude.\"\"\"\n    # Convert tensors to numpy arrays, shape (N, H, W, C)\n    fake = fake.cpu().permute(0, 2, 3, 1).numpy()  # (N, H, W, C)\n    real = real.cpu().permute(0, 2, 3, 1).numpy()  # (N, H, W, C)\n    \n    # Convert to grayscale if multi-channel (RGB)\n    if fake.shape[-1] == 3:  # RGB\n        fake = 0.2989 * fake[..., 0] + 0.5870 * fake[..., 1] + 0.1140 * fake[..., 2]  # (N, H, W)\n        real = 0.2989 * real[..., 0] + 0.5870 * real[..., 1] + 0.1140 * real[..., 2]  # (N, H, W)\n    \n    gsm_scores = []\n    for i in range(fake.shape[0]):\n        fake_img = fake[i]\n        real_img = real[i]\n        \n        # Compute gradient magnitude using Sobel\n        fake_grad = sobel(fake_img)\n        real_grad = sobel(real_img)\n        \n        # Gradient similarity\n        grad_sim = (2 * fake_grad * real_grad + 1e-6) / (fake_grad**2 + real_grad**2 + 1e-6)\n        gsm = np.mean(grad_sim)\n        gsm_scores.append(gsm)\n    \n    return np.mean(gsm_scores)\n\n\ndef test_daass2(model, discriminator, dataloader, criterion, device):\n    model.eval()  # Set the model to evaluation mode\n    discriminator.eval()\n\n    total_vif = 0.0\n    total_fsim = 0.0\n    total_gsm = 0.0\n    total_batches = 0\n\n    with torch.no_grad():\n        print(\"\\n--- Testing ---\")\n\n        for batch_idx, (images, labels, texts) in enumerate(dataloader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass through the generator\n            fake_sketches = model(images, texts)\n\n            # Post-processing: scale Tanh output from [-1, 1] to [0, 1]\n            fake_sketches = (fake_sketches + 1) / 2\n            fake_sketches = fake_sketches.clamp(0, 1)\n\n            # Metrics\n            vif_score = calculate_vif(fake_sketches, labels)\n            fsim_score = calculate_fsim(fake_sketches, labels)\n            gsm_score = calculate_gsm(fake_sketches, labels)\n\n            total_vif += vif_score\n            total_fsim += fsim_score\n            total_gsm += gsm_score\n            total_batches += 1\n\n            # Visualize all images in the first batch only\n            if batch_idx == 0:\n                num_images = images.size(0)\n                fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n\n                if num_images == 1:  # Handle case when batch size is 1\n                    axes = [axes]\n\n                for idx in range(num_images):\n                    input_img = images[idx].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n                    real_img = labels[idx].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n                    gen_img = fake_sketches[idx].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n\n                    axes[idx][0].imshow(input_img)\n                    axes[idx][0].set_title(\"Input Image\")\n                    axes[idx][0].axis(\"off\")\n\n                    axes[idx][1].imshow(real_img)\n                    axes[idx][1].set_title(\"Real Sketch\")\n                    axes[idx][1].axis(\"off\")\n\n                    axes[idx][2].imshow(gen_img)\n                    axes[idx][2].set_title(\"Generated Sketch\")\n                    axes[idx][2].axis(\"off\")\n\n                plt.tight_layout()\n                plt.show()\n\n        # Print average metrics\n        avg_vif = total_vif / total_batches\n        avg_fsim = total_fsim / total_batches\n        avg_gsm = total_gsm / total_batches\n        \n        print(f\"\\nTest Summary  VIF: {avg_vif:.3f} | FSIM: {avg_fsim:.3f} | GSM: {avg_gsm:.3f}\")","metadata":{"_uuid":"3e274897-0317-4551-a81f-ffaf992f1e34","_cell_guid":"0db42377-1f33-4918-8c9f-d90b0fd0fd0e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T06:22:23.184063Z","iopub.execute_input":"2025-05-06T06:22:23.184350Z","iopub.status.idle":"2025-05-06T06:22:23.313713Z","shell.execute_reply.started":"2025-05-06T06:22:23.184329Z","shell.execute_reply":"2025-05-06T06:22:23.312941Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Assuming you have a test DataLoader called `test_loader`:\n\n# Load model weights (if not already loaded)\n# generator_model.load_state_dict(torch.load('generator_epoch_final.pth'))\n# discriminator_model.load_state_dict(torch.load('discriminator_epoch_final.pth'))\n\n# Test the model\ntest_daass2(\n    model=generator_model,\n    discriminator=discriminator_model,\n    dataloader=test_loader,\n    criterion=criterion,\n    device=device\n)","metadata":{"_uuid":"35b7962c-e643-4cd9-8f1d-bfc2d1a1eab0","_cell_guid":"584912f9-8e87-488b-84f8-38a9c424b14c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T06:22:24.771801Z","iopub.execute_input":"2025-05-06T06:22:24.772847Z","iopub.status.idle":"2025-05-06T06:22:29.691622Z","shell.execute_reply.started":"2025-05-06T06:22:24.772821Z","shell.execute_reply":"2025-05-06T06:22:29.690765Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"9df27f3c-23a6-455b-991d-0a1e4071acf0","_cell_guid":"79bb373a-402c-411b-8e10-fe7ce54aa768","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}